# 如何交付机器学习项目
_作者：[**Emmanuel Ameisen**](https://twitter.com/EmmanuelAmeisen)--Insight Data Science 的 AI 负责人、[**Adam Coates**](https://twitter.com/adampaulcoates)--Khosla Ventures 运营合伙人_  
_原文链接：<https://mlpowered.com/posts/how-to-deliver-on-machine-learning-projects/>_  
_2018 年 10 月 4 日  
`Machine Learning`

---
随着机器学习（ML）正在成为各行各业的重要组成部分，市场对机器学习工程师（MLE）的需求急剧增长。
MLE 将机器学习技能与软件工程知识结合起来，为给定的应用找到高性能的模型，并处理随之而来的项目实施问题----从搭建训练基础设施到模型的部署。
同时涌现了大量的在线资源，以培训工程师构建 ML 模型并解决遇到的各种软件工程挑战。
然而，新的 ML 团队最常见的障碍之一是，如何保证机器学习工程的开发进度。

这一挑战最紧迫的原因是，开发新 ML 模型的过程在一开始就具有高度的不确定性。
毕竟，很难知道一个模型在给定的训练运行结束时的性能如何，更不用说预测参数调整或建模选取可以带来怎样的性能提升。

许多类型的专业人士都面临着类似的情况：软件和业务开发人员、寻找产品与市场契合度的初创公司，或者是在有限信息下进行操纵的飞行员。
这些领域都有一种几乎通用的框架，以帮助他们的团队在不确定性中高效地工作：软件开发的“敏捷开发”，创业公司的“精益创业”，以及美国空军的“OODA Loop”。
MLE 也可以遵循类似的框架来应对不确定性，并快速交付优秀的产品。

## ML Engineering Loop（MLE Loop）
在这篇文章中，我们将描述 ML 的“OODA Loop”概念：**MLE Loop**，ML 工程师可以在其中进行迭代：
1. 分析
2. 方法选择
3. 实施
4. 衡量

以快速有效地发现最佳模型并适应未知的情况。
此外，我们为每一个阶段提供具体的提示，以及从整体上优化的建议。

<div align=center><img src="https://s1.ax1x.com/2020/11/09/BH8GkV.png"></div>
<div align=center><h6>MLE Loop</h6></div>

对于 ML 团队来说，成功往往意味着在给定的约束条件下交付一个高性能的模型----例如，一个能达到高预测精度的模型，同时要受到内存使用、推理时间和公平性的约束。
**性能是由任何一个与你的最终产品的成功最相关的指标来定义的**，无论是准确性、速度、输出的多样性等。
为了简单起见，我们选择将“错误率”最小化作为下面的性能指标。

当你刚开始确定一个新项目的范围时，你应该准确定义成功标准，然后将其转化为模型指标。
**在产品方面**，一项服务需要达到什么样的性能水平才能满足用户使用？
例如，如果我们要在新闻平台上向个人用户推荐 5 篇文章，那么我们需要多少篇相关的文章，我们将如何定义相关性？
鉴于这个性能标准和你所掌握的数据，你能建立的最简单的模型是什么？

> 在产品方面，一个服务需要达到什么样的性能水平才能满足用户使用？

ML Engineering Loop 的目的是围绕开发过程建立一套固定的框架，从而简化决策过程，使其仅专注于最重要的后续步骤。
随着从业者经验的进步，这个过程就会变成第二天性，而不断增长的专业知识则可以毫不犹豫地在分析和实施之间快速转变。
也就是说，当不确定性增加时，这个框架对于即使是最有经验的工程师来说也是非常有价值的----例如，当一个模型意外地不能满足需求时，当团队的目标突然被改变时（例如，测试集被改变以反映产品需求的变化），或者当团队的进展在离最终目标不远处停滞不前时。

## 开始
为了引导 MLE Loop 能够正常循环，你应先实现一个最小可运行的框架，这里涉及的不确定性非常小。
通常我们希望尽快地“得到一个结果（不管结果是好还是坏）”----我们只需要建立一个足够小的系统，能让我们评估它的性能并开始迭代就可以了。
这通常意味着做到下面俩点就行：
1. 设置训练、验证和测试数据集
2. 让一个简单的模型正常运作

例如，如果我们要开发一个树木检测器来调查一个地区的树木种群，我们可能会使用类似 [Kaggle 竞赛](https://www.kaggle.com/c/inaturalist-challenge-at-fgvc-2017/data)中的现成数据作为训练集，以及人工收集标记的一组照片作为验证集和测试集。
然后，我们可以在原始像素上运行逻辑回归，或者在训练集的图像上运行一个预训练的网络（如 [ResNet](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)）。
这里的目标不是一次性解决项目，而是能让我们的迭代周期正常开展。
下面是一些小技巧来帮助你做到这一点。

### 技巧：
关于一个好的测试集：
- 由于团队的目标是在测试集上取得好成绩，测试集实际上侧面反映了团队的目标。
因此，测试集应该反映产品或业务的需求。
例如，如果你正在构建一个从自拍中检测皮肤状况的 APP，你可以在任何图像集上进行训练，但要确保你的测试集包含一些像自拍光线不足、质量不佳等图像。
- 改变测试集会改变团队的目标，所以要尽早修正测试集，并且只有当项目、产品或业务目标的变化时，才去修改测试集。
- 争取让测试集和验证集足够大，这样你的性能指标才会足够准确，才能很好地区分不同模型之间的差异。
如果集子太小，你最终只能在嘈杂的结果上做出决策。
- 同样，对于验证集和测试集，您应选择尽可能实用的标签或注释。
一个标签错误的测试集就和一个错误的产品需求差不多。
- 了解人类在鉴别测试集时的表现，或者现有/竞争对手的系统的表现是很有帮助的。
你可以借助这些信息评价自己的模型表现如何，以及可能达到的最佳水平。
- 对于许多任务来说，达到与人类测试性能相当的水平通常是一个长期目标。
在任何情况下，最终的目标是使测试性能尽可能地接近我们假设的最佳性能。

关于验证集和训练集：

- 验证集是团队用于性能测试的代理，他们可以用来调整超参数。
因此，它应该来自与测试集相同的分布，但最好是来自不相干的用户/输入组，以避免数据内部隐藏信息的泄漏。
确保这一点的一个好方法是首先策划一个大的样本池，然后洗牌并在之后将它们分成开发集和测试集。
- 如果你认为生产环境中的数据会很嘈杂，请确保通过使用数据扩充或降级在训练集中解决噪声的问题。
你不能指望一个专门在高清图像上训练的模型能泛化到模糊的图像上。

一旦你有了最初的原型，你应该利用训练、验证和测试集来检查其性能。
这标志着你第一次循环之旅的结束。
总结一下测试的性能与可用产品所需性能之间的差距。
现在是时候开始迭代了!

## 分析
### 识别性能瓶颈
分析阶段就像医疗诊断一样：你配备了一套可以执行的诊断程序，你的目标是对限制你的模型性能的原因提出最有可能的诊断。
在实践中，可能会有许多不同的重叠问题导致当前的结果，但你的目标是**先找到影响最大的问题**，以便迅速解决它们。
不要陷入对每个缺点都要全面理解的泥沼中----首要目标是了解目前最大的影响因素，因为许多较小的问题会随着你对模型的改进而改变甚至消失。

下面，我们列出了一套你将经常使用的常见诊断方法以及一些诊断结果。
并没有具体的方案指导你应该选择哪种诊断程序，但随着在 MLE Loop 中的工作，你可能会逐渐获一些直觉。

最好先从分析模型在训练、验证和测试集上的性能开始。
我们建议在每次实验结束时，再添加上相关代码来做这件事，让自己习惯于每次都看这些数字。
通常情况下，我们会得到：**训练集误差 <= 验证集误差 <= 测试集误差**（前提是每组数据遵循相同的分布）。
利用上次实验中的训练、验证和测试集上的错误率，你可以很快看到这些因素中的哪一个是目前的约束条件。
例如，当训练误差和验证集误差之间的差距很小时，那么你的训练误差就代表了提高性能的瓶颈。

### 诊断
如果**训练集误差**是当前的限制因素，那么可能是由以下问题造成的：

1. 优化算法（例如，深度神经网络的梯度下降）没有精确调整。
看看学习曲线，看看损失函数的值是否在减少。
检查你是否能够过度拟合一个特别小的数据集（例如，在小数据集或单个例子上检查训练时损失是否减少）。
你可以可视化神经元响应的直方图，以检查它们是否饱和（这可能导致梯度消失）。
2. 训练集可能有标记错误或损坏的数据。
在它们被训练算法消耗之前，手动检查一些训练实例的准确性。
3. 模型可能太小或缺乏表达能力。
例如，如果你使用线性回归来处理一个高度非线性的问题，你的模型根本无法很好地拟合数据。
我们称这个模型是高偏差或欠拟合的。

如果**验证集误差**是当前的限制因素，这可能是由下面一系列类似的问题引起的：

1. 模型可能太大，或表现力太强，或不够规范化。
我们称模型是高方差或过拟合的。
2. 训练数据不足，没法使模型学习到一个足够好的底层模式。
3. 训练数据的分布与验证或测试数据分布不匹配。
4. 模型的超参数设置得不好。
如果你在超参数上搜索（如特征集、正则化项等），可能是搜索方法没有找到好的选择。
5. 你的模型中假设的“归纳先验”与实际数据分布的匹配度很差。
例如，如果你使用的是最近邻方法，但数据的分布呈线性函数时，你可能会泛化不好，除非你有更多的训练数据。

如果**测试集误差**是当前的限制因素，这通常是由于验证集太小，或者团队在多次实验的过程中不经意间对验证集过度拟合。

对于上述任何一种情况，你都可以通过手动检查一些**随机的例子集**来了解模型失败的原因（一般不应该对测试集这样做，以避免这些例子间接“训练”了你的模型）。

1. 尝试通过**可视化数据**来识别常见的错误类型。
然后仔细检查这些例子，并对每种类型的错误发生的频率进行统计。
对于分类，查看混淆矩阵，并确定你表现最差的类。
然后，你可以专注于解决错误最多的类型。
2. 有些例子可能是错误的标签或同一个例子有多个合理的标签。
3. 模型鉴别某些例子可能比其他例子更难，或者可能缺少良好决策所需的上下文。
如果有几组例子的错误率相当，将一些标记为“难以鉴别”可能会引导你的工作以更容易实现目标。
同样，将一些标记为“容易鉴别”可能会提示你，你的系统中存在一些微不足道的错误，导致它错过了容易鉴别的案例。
这有点像在不同的数据子集上估计“最佳错误率”，然后选择在进步空间最大的子集上进行深入研究。

请注意，上面的很多诊断，我们都可以直接进行改善。
比如，如果训练数据太少，那就多弄些训练数据吧！
我们还是觉得在心理上把分析阶段和选择阶段分开是很有用的（如下图），因为很容易陷入尝试随机的方法，而没有真正挖掘出根本问题。
此外，以开放的心态勤奋地回到错误中进行分析，往往会发现有用的见解，从而改善你的决策。

#### 例子
以 Insight 为例，当 AI 研究员 Jack Kwok 在构建一个图像分割系统来帮助灾后恢复时，他注意到，虽然他的图像分割模型在卫星图像的训练集上表现良好，但在一些验证集上表现不佳，这些验证集中包含了被飓风淹没城市的图像。
原因是有飓风的图像清晰度较低，比训练数据更模糊。
所以在训练流程中增加一个额外的增强步骤，对图像进行模糊处理，将有助于缩小训练集和验证集性能之间的差距。

对于语音识别系统来说，对验证集进行深入的错误分析可能会发现，具有强烈口音的说话者与大多数用户的口音截然不同，代表了一定比例的错误数量。
然后，人们可以检查训练集，看看类似的口音是否被训练算法很好地代表、正确地标记和成功地拟合。
对一些用户群体的代表性不足或标记错误是机器学习中出现偏差的常见原因之一。
对于谷歌的语音系统，它的解决方案是主动向口音较重的用户征集额外的训练数据。

## 选择方法
### 找到解决瓶颈最简单的方法
在分析之后，你对模型犯了什么样的错误以及哪些因素阻碍了性能的发挥有一个很好的认识。
对于一个给定的诊断，可能会有几个潜在的解决方案，下一步就是枚举并**确定优先级**。

上面的一些诊断会直接引出一些潜在的解决方案。
例如，如果你的优化器调参错误，你可以尝试不同的步长大小，或者考虑完全切换优化算法。
如果训练数据集太小，收集更多的训练数据可能是一个相当快速和简单的解决方案。

我们建议 ML 工程师和他们的团队尽可能多地枚举可行的想法，然后偏向于选择**简单、快速的解决方案**。
如果现有的解决方案可行（例如，使用已经在你的工具箱中实现的另一种优化算法），就从它开始。
虽然更复杂的方法可能看起来会一次性完成更多的工作，但我们经常发现，许多能快速迭代的改进所带来的收益远超实施最新技术或定制解决方案，因为从新技术的实施和方案的定制到最终问题的解决，中间要花费太多的时间。
如果你可以在标记 1000 个数据点或研究一种前沿的无监督学习方法之间进行选择，我建议你应该选择收集和标记数据。
如果你能从一些简单的启发式特征开始，就去做吧。

  > 许多能快速迭代的改进所带来的收益远超实施最新技术或定制解决方案，因为从新技术的实施和方案的定制到最终问题的解决，中间要花费太多的时间。

### 技巧
根据你的诊断结果，这里有一些常见的解决方案。

如果你需要调整优化器以更好地适应数据：

- 对于数值优化器，尝试调整学习率或动量设置。
从一个小的动量（0.5）开始通常是最容易工作的。
- 尝试不同的初始化策略，或者从预训练的模型开始。
- 尝试一种更容易调整的模型类型。
在深度学习中，残差网络或具有批处理归一化的网络可能更容易训练。

如果模型无法很好地拟合训练数据：

- 使用更大或更具表现力的模型类。
例如，当使用决策树时，你可以使树更深。
- 检查模型在训练集上出错的例子，是否有标签错误、缺失字段等。
在训练数据清理上投入时间可以显著提高结果。

如果模型无法对验证集进行泛化:

- 添加更多的训练数据。
注意，专注于**添加与验证集中看到的错误类别相似的训练示例可能很重要**。
- 用从真实训练实例生成的新样本来增强你的数据。
例如，如果你注意到你的树木检测器在雾状图像上的表现一直很差，那么就用 OpenCV 来添加一个增强步骤，使你的图像看起来有点雾状。
- 在更宽或更细粒度的超参数范围内搜索，以确保你找到在验证集上表现最好的模型。
- 尝试不同形式的正则化（如权重衰减，神经网络的 dropout，或决策树的修剪）。
- 尝试不同的模型类。
不同类型的模型会改变你对数据的拟合程度和泛化程度，所以很难知道什么时候会有效。
深度学习的一个优势是，有广泛的"积木式"神经网络组件，你可以轻松尝试。
如果你使用的是传统的模型（如决策树或高斯混合物模型），切换模型类的工作要复杂得多。
如果新模型所体现的假设更正确，那么改变可能会有帮助----但先尝试更简单的东西可能会更好。

## 实施
### 只搭建你需要的东西，并且要快速地完成
你知道下一步要尝试什么，而且你已经把它简单化了，现在只是实施的问题......
“只是”。这个阶段的目标是快速**原型化**，这样你就可以**评估**结果，从中**学习**，并快速地回到循环中去。
出于这个原因，我们建议专注于构建只是你当前实验所需的东西。
请注意，虽然你的目标是快速进入学习循环而不是打磨一切，但你的工作仍然需要**正确**，因此你应该经常检查代码是否按照预期工作。

### 技巧
  > 大多数人都高估了收集和标注数据所需的成本，而低估了在数据短缺的环境中解决问题的艰辛。

在收集和标注数据时：

- **定期查看你的数据**。
看原始数据，看预处理后的数据，看标签。
这一点我们怎么强调都不过分！
只要在流水线的每一步都盯着你的数据，你就能发现很多错误。
每个阶段，Insight 的 AI 研究员都会在解决计算机视觉问题时，捕捉到与数据清洗、边界框坐标和图像分割有关的错误。
- 标签和清洗数据是一项常见的任务。
大多数人都高估了收集和标注数据所需的成本，而低估了在数据短缺的环境中解决问题的艰辛。
一旦你掌握了节奏，你可以轻松地每分钟对 20 张图片进行分类标记。
对于一个拥有 1200 张图片的数据集，你是愿意花一个小时去标注图片，再花一个小时去解决这个的简单分类问题，还是花 3 周时间去让一个模型从 5 个例子中学习？

当你对一个新的模型进行编码时，从现有的类似实现开始。
现在许多研究论文都有免费提供的代码----所以在重新实现论文中的一个想法之前，尽量先获取代码，因为论文中往往有一些没有记录的细节。
这将为您节省数小时 or 天的工作。
如果可能，对于任何问题，我们建议通过下面这些步骤来实施你的项目：

1. 找到一个解决类似问题的模型的实现。
2. 保持现有模型的条件下，在本地重现该实现（相同的数据集和超参数）。
3. 慢慢调整模型的实现和数据管道，以满足你的需求。
4. 重写任何需要的部分。

编写测试来检查你的梯度、张量值、输入数据和标签是否正确格式化。
在最初设置模型时就这样做，这样一来，如果你发现了错误，处理一次之后，就再也不用管它了。

## 评估
打印出你的测试结果和你需要的任何其他指标（例如，生产限制），以决定模型是否准备好出货。

如果模型的性能有了一定的改善，你可能已经走上了正轨。
在这种情况下，现在可能是一个很好的时间，来清理任何保持模型良好工作的组件，并确保实验可以由你团队中的其他人重现。

另一方面，如果性能变得更差或没有足够的改进，你需要决定是再试一次（回到分析阶段）还是放弃你目前的想法。
如果 **ML 循环的每个周期都比较简便**，那么这些决定就比较容易做出：
你没有投入太多精力让你的代码变得完美，而且再一次尝试也不会花费太长时间----所以你可以根据想法的风险和价值而不是沉没成本来决定该怎么做。

### 技巧
- 有用的性能指标包括 ML 方面的准确率和损失，以及商业价值指标（例如，我们在前 5 名中推荐正确的文章的频率有多高？）
请记住，后者的指标才是最终重要的，因为它们是决定你正在构建的东西是否真的有用的指标。
如果你的测试指标(由你的 ML 代码优化而来的)与你的业务指标有偏差，这个测量周期的结束是停止并考虑更改优化标准或测试集的好时机。
- 因为不管怎样，你都会在每个开发循环的末尾打印出您的指，你也可以很方便地计算其他指标，这在分析阶段很有用，并且可以帮助你决定是否要继续当前的想法。
- 通常你最终会建立一个“仪表盘”，上面有你的测试指标和业务指标，以及其他有用的数据，你可以在每个实验结束时看到。
这在团队工作时特别有用。

## 优化你的循环
上面的 ML 工程循环将帮助你有条不紊地朝着更好的模型迈进，尽管任务本身具有不确定性。
当然事情并不总是令人愉快的，不幸的是----你仍然需要培养自己在每个阶段做出正确选择的能力，比如识别性能瓶颈，决定尝试哪些解决方案，如何正确地实现它们，以及如何衡量应用的性能。
你还需要善于快速迭代。
因此，你也应该花时间去思考如何提高迭代的*质量*和*速度*，这样你才能在每个周期中取得最大的进步，你才能快速完成多次迭代。

### 技巧
- 如果你的分析阶段拖慢了你的速度，可以创建一个脚本，总结实验结果，收集训练集和验证集的错误，并将它们很好地格式化。
这个经常使用的诊断输出的“仪表盘”可以帮助你克服类似这样的想法：“唉！我又要手动运行那个分析......我还是试试这个随机的解决方案吧。”
- 如果你觉得自己在为尝试什么而手忙脚乱，就随便选一个先做吧。
一下子想做太多事情，会让你的速度变慢。
你有时可以在你的实验进行地同时回来尝试另一个想法！
- 收集数据是提高性能的常见方法。
如果获得更多的数据听起来很痛苦，但真的会有所改变，那么将部分精力投资于一些工具，以使数据更容易收集、清理和标记。
- 如果你在诊断瓶颈或选择下一步尝试的哪种模型上感到困顿，可以考虑联系专家。
领域专家通常可以在错误分析过程中提供有用的见解（例如，指出使某些案例变得困难或容易的微妙区别），你也可以从研究论文或有经验的 ML 从业者哪里找到一些创造性的解决方案，来添加到你的候选尝试列表中（如果你能提供项目目前分析结果地细节，其他人将能更好地帮助你）。
- 良好的实施技能很重要，清洁的代码可以防止 bug 的发生。
也就是说，由于有很大一部分想法会失败，所以在迭代的过程中，你可以随意地砍掉你的实验代码，扔掉失败的代码。
一旦你觉得你已经取得了有用的进展，在下一次循环之前，你可以对代码施加一些约束并进行清理。
- 如果你的实验耗时过长，可以考虑花一些时间寻找优化代码的方法。
或者和系统专家讨论如何让你的训练速度更快。
在有疑问的时候，购买升级的 GPU 或并行运行更多的实验是解决 ML 实验中“等不及”问题的老办法。

与其他决策一样，只有当这些选项能解决当前的痛点时，才会在这些选择上下功夫。
有些团队花了太多时间去构建“完美”的框架，却发现真正头疼的问题在别的地方。

## 结论
ML 项目本质上是不确定的，我们上面推荐的方法是为了给你提供一个指导。
虽然当你的实验命运不确定时，你很难要求自己达到一个具体的准确率目标，但你至少可以要求自己完成那个错误分析，拟定一个想法清单，对它们进行编码，并看看它的效果如何。
根据 Emmanuel 和 Adam 的经验，我们不要想着一蹴而就，而要坚持不懈地关注渐进式的进展，这样才可以在研究和应用中取得非凡的成果。
这种思路改变了许多团队，让无数研究员在尖端项目上取得了成果。

---
[返回目录](https://github.com/datugou/Article_Translation/tree/master/LEARNING_data_science)
