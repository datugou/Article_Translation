# 如何交付机器学习项目
_作者：[**Emmanuel Ameisen**](https://twitter.com/EmmanuelAmeisen)--Insight Data Science 的 AI 负责人、[**Adam Coates**](https://twitter.com/adampaulcoates)--Khosla Ventures 运营合伙人_  
_原文链接：<https://mlpowered.com/posts/how-to-deliver-on-machine-learning-projects/>_  
_2018 年 10 月 4 日  
`Machine Learning`

---
随着机器学习（ML）正在成为各行各业的重要组成部分，市场对机器学习工程师（MLE）的需求急剧增长。
MLE 将机器学习技能与软件工程知识结合起来，为给定的应用找到高性能的模型，并处理随之而来的项目实施问题--从搭建训练基础设施到模型的部署。
同时涌现了大量的在线资源，以培训工程师构建 ML 模型并解决遇到的各种软件工程挑战。
然而，新的 ML 团队最常见的障碍之一是，如何保证机器学习工程的开发进度。

这一挑战最紧迫的原因是，开发新 ML 模型的过程在一开始就具有高度的不确定性。
毕竟，很难知道一个模型在给定的训练运行结束时的性能如何，更不用说预测参数调整或建模选取可以带来怎样的性能提升。

许多类型的专业人士都面临着类似的情况：软件和业务开发人员、寻找产品与市场契合度的初创公司，或者是在有限信息下进行操纵的飞行员。
这些领域都有一种几乎通用的框架，以帮助他们的团队在不确定性中高效地工作：软件开发的“敏捷开发”，创业公司的“精益创业”，以及美国空军的“OODA Loop”。
MLE 也可以遵循类似的框架来应对不确定性，并快速交付优秀的产品。

## ML Engineering Loop（MLE Loop）
在这篇文章中，我们将描述 ML 的“OODA Loop”概念：**MLE Loop**，ML 工程师可以在其中进行迭代：
1. 分析
2. 方法选择
3. 实施
4. 衡量

以快速有效地发现最佳模型并适应未知的情况。
此外，我们为每一个阶段提供具体的提示，以及从整体上优化的建议。

<div align=center><img src="https://s1.ax1x.com/2020/11/09/BH8GkV.png"></div>
<div align=center><h6>MLE Loop</h6></div>

对于 ML 团队来说，成功往往意味着在给定的约束条件下交付一个高性能的模型--
例如，一个能达到高预测精度的模型，同时要受到内存使用、推理时间和公平性的约束。
**性能是由任何一个与你的最终产品的成功最相关的指标来定义的**，无论是准确性、速度、输出的多样性等。
为了简单起见，我们选择将“错误率”最小化作为下面的性能指标。

当你刚开始确定一个新项目的范围时，你应该准确定义成功标准，然后将其转化为模型指标。
**在产品方面**，一项服务需要达到什么样的性能水平才能满足用户使用？
例如，如果我们要在新闻平台上向个人用户推荐 5 篇文章，那么我们需要多少篇相关的文章，我们将如何定义相关性？
鉴于这个性能标准和你所掌握的数据，你能建立的最简单的模型是什么？

> 在产品方面，一个服务需要达到什么样的性能水平才能满足用户使用？

ML Engineering Loop 的目的是围绕开发过程建立一套固定的框架，从而简化决策过程，使其仅专注于最重要的后续步骤。
随着从业者经验的进步，这个过程就会变成第二天性，而不断增长的专业知识则可以毫不犹豫地在分析和实施之间快速转变。
也就是说，当不确定性增加时，这个框架对于即使是最有经验的工程师来说也是非常有价值的----例如，当一个模型意外地不能满足需求时，当团队的目标突然被改变时（例如，测试集被改变以反映产品需求的变化），或者当团队的进展在离最终目标不远处停滞不前时。

## 开始
为了引导 MLE Loop 能够正常循环，你应先实现一个最小可运行的框架，这里涉及的不确定性非常小。
通常我们希望尽快地“得到一个结果（不管结果是好还是坏）”----我们只需要建立一个足够小的系统，能让我们评估它的性能并开始迭代就可以了。
这通常意味着做到下面俩点就行：
1. 设置训练、验证和测试数据集
2. 让一个简单的模型正常运作

例如，如果我们要开发一个树木检测器来调查一个地区的树木种群，我们可能会使用类似 [Kaggle 竞赛](https://www.kaggle.com/c/inaturalist-challenge-at-fgvc-2017/data)中的现成数据作为训练集，以及人工收集标记的一组照片作为验证集和测试集。
然后，我们可以在原始像素上运行逻辑回归，或者在训练集的图像上运行一个预训练的网络（如 [ResNet](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)）。
这里的目标不是一次性解决项目，而是能让我们的迭代周期正常开展。
下面是一些小技巧来帮助你做到这一点。

### 小贴士：
关于一个好的测试集：
- 由于团队的目标是在测试集上取得好成绩，测试集实际上侧面反映了团队的目标。
因此，测试集应该反映产品或业务的需求。
例如，如果你正在构建一个从自拍中检测皮肤状况的 APP，你可以在任何图像集上进行训练，但要确保你的测试集包含一些像自拍光线不足、质量不佳等图像。
- 改变测试集会改变团队的目标，所以要尽早修正测试集，并且只有当项目、产品或业务目标的变化时，才去修改测试集。
- 争取让测试集和验证集足够大，这样你的性能指标才会足够准确，才能很好地区分不同模型之间的差异。
如果集子太小，你最终只能在嘈杂的结果上做出决策。
- 同样，对于验证集和测试集，您应选择尽可能实用的标签或注释。
一个标签错误的测试集就和一个错误的产品需求差不多。
- 了解人类在鉴别测试集时的表现，或者现有/竞争对手的系统的表现是很有帮助的。
你可以借助这些信息评价自己的模型表现如何，以及可能达到的最佳水平。
- 对于许多任务来说，达到与人类测试性能相当的水平通常是一个长期目标。
在任何情况下，最终的目标是使测试性能尽可能地接近我们假设的最佳性能。

关于验证集和训练集：

- 验证集是团队用于性能测试的代理，他们可以用来调整超参数。
因此，它应该来自与测试集相同的分布，但最好是来自不相干的用户/输入组，以避免数据内部隐藏信息的泄漏。
确保这一点的一个好方法是首先策划一个大的样本池，然后洗牌并在之后将它们分成开发集和测试集。
- 如果你认为生产环境中的数据会很嘈杂，请确保通过使用数据扩充或降级在训练集中解决噪声的问题。
你不能指望一个专门在高清图像上训练的模型能泛化到模糊的图像上。

一旦你有了最初的原型，你应该利用训练、验证和测试集来检查其性能。
这标志着你第一次循环之旅的结束。
总结一下测试的性能与可用产品所需性能之间的差距。
现在是时候开始迭代了!

## 分析
### 识别性能瓶颈
分析阶段就像医疗诊断一样：你配备了一套可以执行的诊断程序，你的目标是对限制你的模型性能的原因提出最有可能的诊断。
在实践中，可能会有许多不同的重叠问题导致当前的结果，但你的目标是**先找到影响最大的问题**，以便迅速解决它们。
不要陷入对每个缺点都要全面理解的泥沼中----首要目标是了解目前最大的影响因素，因为许多较小的问题会随着你对模型的改进而改变甚至消失。

下面，我们列出了一套你将经常使用的常见诊断方法以及一些诊断结果。
并没有具体的方案指导你应该选择哪种诊断程序，但随着在 MLE Loop 中的工作，你可能会逐渐获一些直觉。

最好先从分析模型在训练、验证和测试集上的性能开始。
我们建议在每次实验结束时，再添加上相关代码来做这件事，让自己习惯于每次都看这些数字。
通常情况下，我们会得到：**训练集误差 <= 验证集误差 <= 测试集误差**（前提是每组数据遵循相同的分布）。
利用上次实验中的训练、验证和测试集上的错误率，你可以很快看到这些因素中的哪一个是目前的约束条件。
例如，当训练误差和验证集误差之间的差距很小时，那么你的训练误差就代表了提高性能的瓶颈。

## 诊断和诊断
如果**训练集误差**是当前的限制因素，那么可能是由以下问题造成的：

优化算法（例如，深度神经网络的梯度下降）没有精确调整。
看看学习曲线，看看损失函数的值是否在减少。
检查你是否能够过度拟合一个特别小的数据集（例如，在小数据集或单个例子上检查训练时损失是否减少）。
你可以可视化神经元响应的直方图，以检查它们是否饱和（这可能导致梯度消失）。
训练集可能有标记错误或损坏的数据。在你的代码阶段，在它们被训练算法消耗之前，手动检查一些训练实例。
模型可能太小或缺乏表达能力。例如，如果你使用线性回归来处理一个高度非线性的问题，你的模型根本无法很好地拟合数据。我们说这个模型是高偏或低拟合的。
如果开发集误差是当前的限制因素，这可能是由上述一系列类似的问题引起的。

模型可能太大，或表现力太强，或不够规范化。我们说模型有高方差或过拟合。
没有足够的训练数据来学习一个好的底层模式模型。
训练数据的分布与开发或测试数据分布不匹配。
模型的超参数设置得不好。如果你在超参数上搜索（如特征集、正则化项等），可能是搜索方法没有找到好的选择。
你的模型中编码的 "归纳先验 "与数据的匹配度很差。例如，如果你使用的是最近邻方法，当数据自然由线性函数表示时，你可能会泛化不好，除非你有更多的训练数据。
如果测试集误差是当前的限制因素，这通常是由于开发集太小，或者团队在许多实验过程中对开发集过度拟合。

对于上述任何一种情况，你都可以通过手动检查你的模型出错的随机例子集来了解你的模型的失败（你一般不应该对测试集这样做，以避免在这些例子上 "训练 "你的系统）。

尝试通过可视化数据来识别常见的错误类型。然后仔细检查这些例子，并对每种类型的错误发生的频率进行统计。对于分类，查看混淆矩阵，并确定你表现最差的类。然后，你可以专注于解决占错误最多的错误类型。
有些例子可能是错误的标签或有多个合理的标签。
有些例子可能比其他例子更难，或者可能缺少一个好的决定所需的上下文。如果有几组错误同样常见，将一些例子标记为 "非常难 "可能会帮助你将努力指向较低的悬果。同样，将一些标记为 "非常容易 "可能会提示你，你的系统中存在一个微不足道的错误，导致它错过了容易的案例。这有点像在不同的数据子集上估计 "最佳错误率"，然后在进步空间最大的子集上深入研究。
请注意，上面的很多诊断都有直接明显的反应。比如，如果训练数据太少，那就多弄些训练数据吧! 我们还是觉得在心理上把分析阶段和选择阶段分开是很有用的（如下图），因为很容易陷入尝试随机的方法，而没有真正挖掘出根本问题。此外，以开放的心态勤奋地回到错误分析中，往往会发现有用的见解，从而改善你的决策。




## ~~章节题目~~
~~内容~~
<img src="https://s1.ax1x.com/2020/10/14/054e6e.png" alt="054e6e.png" border="0" />
<div align=center><img src="https://s1.ax1x.com/2020/10/14/054e6e.png"></div>
<div align=center><h6>在极端情况下，每个功能区/产品由7个人组成</h6></div>
---
[返回目录](https://github.com/datugou/Article_Translation/tree/master/LEARNING_data_science)
