# ~~题目~~
_作者：**Isaac Caswell, Bowen Liang**--谷歌研究部软件工程师_  
_原文链接：<https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html>_  
_2020 年 6 月 8 日_
`Machine Learning` `Translation`

---
机器学习（ML）的进步推动了自动翻译的改进，包括在 2016 年推出的 [GNMT 神经翻译模型](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)，使 100 多种语言的翻译质量得到了极大的改善。
尽管如此，除了一些特殊的翻译任务外，即使是最先进的翻译系统，其表现都明显落后于人类。
而且，虽然研究界已经开发出了对西班牙语和德语等高资源语言成功的技术，对这些语言存在大量的训练数据，但在约鲁巴语或马拉雅拉姆语等低资源语言上的表现仍有很多需要改进的地方。
在一些特定研究环境中，许多技术对低资源语言已经展示出了显著的提升（例如，[WMT 评估活动](http://www.statmt.org/wmt20/)），但是这种在较小的、公开数据集上得到的结果，可能很难过渡到大型的、网络爬取的数据集上。。

在这篇文章中，我们分享了最近谷歌翻译在质量方面上取得的一些进展，特别是对于那些低资源的语言，通过综合和扩展各种最近的研究进展，将它们大规模地应用于嘈杂的、网络挖掘的数据中。
这些技术涵盖了对模型架构和训练的改进，对数据集中噪声的改进处理，通过 [M4 建模](https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html)增加了多语言迁移学习，以及单语言数据的使用。
在所有 100 多种语言中的质量改进结果如下图所示，其中 [BLEU 得分](https://en.wikipedia.org/wiki/BLEU)平均增加了 5 分。


<div align=center><h6>谷歌翻译模型自 2006 年成立后的 BLEU 得分。在动画的结尾处强调了自去年实施新技术以来得到的改进。</h6></div>

## 高资源和低资源语言翻译的进步
**混合模型架构**：四年前，我们引入了基于 RNN 的 [GNMT 模型](https://arxiv.org/abs/1609.08144)，该模型对翻译质量有了很大的提高，并使 Google Translate 能够覆盖更多的语言。
根据我们[对模型性能的不同方面进行解耦的工作](https://arxiv.org/abs/1804.09849)，我们替换了原来的 GNMT 系统，而是用一个 [transformer](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) 编码器和一个 RNN 解码器来训练模型，在 [Lingvo](https://arxiv.org/abs/1902.08295)（TensorFlow 框架）中实现。
[transformer 模型已经被证明在机器翻译方面的表现要比 RNN 模型更有效[(https://arxiv.org/pdf/1706.03762.pdf)，但我们的工作表明，这些质量的提升大部分来自于 transformer 编码器，而 transformer 解码器并没有明显优于 RNN 解码器。
由于 RNN 解码器在推理时间上要快得多，我们在将其进行了各种优化，然后与 transformer 编码器进行耦合。
所得到的混合模型质量更高，训练时更稳定，并且表现出更低的延迟。

**网络抓取**：神经机器翻译（NMT）模型是使用翻译句子和文档的样本进行训练的，这些样本通常是从公共网络上收集的。
与基于短语的机器翻译相比，[NMT 模型的翻译效果对数据质量更加敏感](https://www.aclweb.org/anthology/W18-2709.pdf)。
因此，我们用一个新的数据挖掘机取代了之前的数据收集系统，它更注重精确率而不是召回率，这使得我们可以从公共网络中收集更高质量的训练数据。
此外，我们将 14 种大型语言对的网络爬虫从基于字典的模型换成了基于嵌入的模型，在不损失精度的情况下，平均增加了 29% 收集到的句子数量。

**对数据噪声进行建模**：具有显著噪声的数据不仅是冗余的，而且会降低在其上训练的模型的质量。
为了解决数据噪声问题，我们利用我们在[去噪 NMT 训练](https://www.aclweb.org/anthology/W18-6314/)上取得的成果：首先在噪声数据上训练一个初步的模型，再用干净数据对模型微调，然后用这个模型给每个训练样本进行打分。
然后，我们将翻译模型的训练视为“[课程学习问题](https://arxiv.org/abs/1908.10940)”--模型一开始在全部数据上训练，然后逐渐在较小和较干净的子集上训练。

## 使低资源语言受益的进步
**回译（Back-Translation）**：回译广泛应用于最新的机器翻译系统中，特别适用于那些训练资源匮乏的语言。
该技术使用合成的平行句对数据（原句子与其翻译结果成对）来增强训练过程，其中原语言的句子是由人类编写的，但是它们的翻译则是由神经翻译模型生成的。
通过将回译整合到 Google Translate 中，对于网络上资源较少的语言，我们可以更好地利用单语言文本数据来训练我们的模型。
通常在机器翻译领域中，低资源翻译模型的表现不佳，因此回译的应用对于提高模型输出的流利性尤其有用。

**M4 建模**：M4 是一种对资源匮乏的语言特别有用的技术，它使用单一的大型模型在所有语言和英语之间进行翻译。
这允许大规模的迁移学习。
例如，像意第绪语这样的资源较少的语言具有与多种其他相关日耳曼语系语言（例如德语，荷兰语，丹麦语等）以及将近一百种可能不共享已知的语言联系，但可以为模型提供有用的信号。
例如，像意第绪语这样的资源较少的语言，可以和其他相关的日耳曼语（例如德语，荷兰语，丹麦语等）共同训练，从而提升翻译性能；
也可以和其他一百多种语言一起训练，这些语言之间可能存在一些未知的联系，从而为模型提供有用的信号。

## ~~章节题目~~
~~内容~~
<img src="https://s1.ax1x.com/2020/10/14/054e6e.png" alt="054e6e.png" border="0" />
<div align=center><img src="https://s1.ax1x.com/2020/10/14/054e6e.png"></div>
<div align=center><h6>在极端情况下，每个功能区/产品由7个人组成</h6></div>
---
[返回目录](https://github.com/datugou/Article_Translation/tree/master/LEARNING_data_science)
