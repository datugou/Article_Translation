# 软件 2.0
_作者：[**Andrej Karpathy**](https://medium.com/@karpathy)_  
_原文链接：<https://medium.com/@karpathy/software-2-0-a64152b37c35>_  
_2017 年 11 月 12 日_  
`machine learning` `software development` `AI`

---
我有时会看到人们将神经网络称为“机器学习工具箱中的一个工具”而已。
它们有一些优点和缺点，在这里或那里发挥作用，有时你可以用它们来赢得 Kaggle 比赛。
不幸的是，这种解释完全是只见树木，不见森林。
神经网络不仅仅是另一种分类器，它们代表了我们编写软件方式的根本开始发生转变。
这就是软件 2.0。

软件 1.0 的“经典堆栈”就是我们大家熟悉的--用 Python、C++ 等语言编写。
由程序员写给计算机的明确指令组成。
通过编写每一行代码，程序员控制程序做出期望的行为。

相比之下，软件 2.0 则用更抽象的、而且对人类不友好的语言来编写，比如神经网络的权重。
没有人参与编写这部分代码，因为有太多权重（典型的网络可能有几百万个），直接在权重方面编码有点难（我试过）。

<div align=center><img src="https://s1.ax1x.com/2020/10/19/0xF4cF.png" width="500"></div>

相反，我们的方法是在所需程序的行为上指定一些目标（例如，“满足特定输入输出的数据集”，或者“赢得一盘围棋”），写出一个大致的代码框架（例如神经网络框架），先确定一个包含许多可能实现该功能的程序的集合，然后利用我们所收集到的数据资源在这个集合中搜索出一个最有效的程序。
在神经网络的具体案例中，我们将搜索限制在程序空间的一个连续子集上，在这个子集上，搜索过程可以通过反向传播和随机梯度下降来快速实现。

<div align=center><img src="https://s1.ax1x.com/2020/10/19/0xZRQf.png" width="500"></div>

事实证明，很大一部分现实世界的问题都具有这样的特性：收集数据（或者更一般地说，确定一个理想的行为）比编写明确的程序要容易得多。
在这些情况下，程序员会分成两个团队。
2.0 的程序员手动整理、维护、调整、清理和标注数据集；
每个标注的数据都将为最终的系统做出贡献，因为数据集会通过优化被编译到软件 2.0 的代码中。
同时，1.0 的程序员则负责维护边缘的代码，如工具、分析、可视化、标签界面、基础设施和训练。

## 正在进行的过渡
让我们简单研究一下这种正在进行的过渡的一些具体例子。
在过去几年里，我们看到了这些领域中的改进，我们放弃试图通过编写显式代码来解决一个复杂的问题，而是将代码过渡到 2.0。

**视觉识别**的任务最初是由工程化的特征组成，然后在上面加上一点机器学习算法（例如，SVM）。
从那时起，我们通过获取大型数据集（如 ImageNet）并在卷积神经网络架构的空间中搜索，发现了更强大的视觉特征。
最近，我们在架构的选择上，也不用自己手动选择，而是同样应用搜索算法来确定（[详见](https://arxiv.org/abs/1703.01041)）。

**语音识别**过去涉及大量的预处理、高斯混合物模型和隐藏马尔科夫模型，但[今天几乎完全由神经网络框架组成](https://github.com/syhw/wer_are_we)。
有一句非常相关的、经常被引用的幽默言论，出自于 1985 年，Fred Jelinek，他说：“每当我解雇一个语言学家，我们的语音识别系统的性能就会上升”。

**语音合成**在历史上一直用各种拼接机制来处理，但如今最先进的大型卷积网络（如 [WaveNet](https://deepmind.com/blog/wavenet-launches-google-assistant/)），可以生成原始音频信号。

**机器翻译**通常采用基于短语的统计技术，但基于神经网络的算法正在迅速占据主导地位。
我最喜欢的架构是在弱监督（或[完全无监督](https://arxiv.org/abs/1710.11041)）的环境中训练的[多语言模型](https://arxiv.org/abs/1611.04558)，这些模型可以从任何源语言翻译到任何目标语言。

**游戏**。手动编写规则的下围棋程序有很久的历史了，但基于神经网路的 [AlphaGo Zero](https://deepmind.com/blog/alphago-zero-learning-scratch/)（一个通过棋盘原始状态预测下一步棋的卷积网络）已经成为迄今为止最强的“围棋选手”。
我预计在其他领域也会看到非常类似的结果，比如 [DOTA 2](https://blog.openai.com/more-on-dota-2/)，或者[星际争霸](https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/)等游戏。

**数据库**。人工智能之外的更多传统系统也出现了转型的早期苗头。
例如，“[一个学习索引结构的案例](https://arxiv.org/abs/1712.01208)”用神经网络取代了数据管理系统的核心组件，在速度上比缓存优化的 B-Trees 结构快了 70%，同时节约了一个数量级的内存。

你会注意到，我上面的很多内容都涉及到谷歌所做的工作。
这是因为谷歌目前正处于将传统代码升级到软件 2.0 代码的前沿。
“[一个模型解决所有问题](https://arxiv.org/abs/1706.05137)”描绘了一个未来可能实现的草图：掌握各个领域所需的能力将合并成一个一致的，它可以理解整个世界。

## 软件 2.0 的优势
为什么我们要优先将复杂的程序移植到软件 2.0 中？
显然，一个最直接的理由就是，它们在实践中更好用。
不过，还有很多其他方便的原因让我们更喜欢应用这种堆栈。
来看看软件 2.0（想象成一个卷积网络）与软件 1.0（想象成一个生产级的 C++ 代码库）相比的一些好处。
软件2.0是：

**计算上同质**。
一个典型的神经网络就像一个三明治结构，通常只有两个操作：一阶矩阵乘法和零阈值（ReLU）。
将其与经典软件的指令集进行比较，后者更加异构和复杂。
在软件 2.0 中，由于你只需要用软件 1.0 的少量代码来实现核心的计算单元（例如矩阵乘法），那么确保程序的正确性和性能就容易得多。

**更容易封装到硅芯片中**。
由于操作神经网络的指令集相对较小，因此很容易将这些指令集封装到硅芯片中，使这些网络可以直接在硅片上实现，例如使用定制的 [ASIC](https://www.forbes.com/sites/moorinsights/2017/08/04/will-asic-chips-become-the-next-big-thing-in-ai/#7d6d7c0511d9)、[神经形态芯片](https://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-learningor-obscurity)等。
当低功耗的智能在我们身边普及的时候，世界将发生改变。
例如，小型、廉价的芯片可以带有一个预先训练好的卷积网络、一个语音识别器和一个 WaveNet 语音合成网络，所有这些都集成在一个小的原脑中，你可以把它连接到其它东西上。

**恒定的运行时间**。
一个典型的神经网络正向传递的每一次迭代都需要完全相同的 FLOPS（每秒浮点运算次数）。
相对于一些庞大的 C++ 代码库，程序执行过程中可能采取的不同执行路径，软件 2.0 的运行时间几乎是没有变化的。
当然，你可以使用动态的计算图，但执行流通常还是受到很大的限制。
这样我们也可以保证在程序陷入意外的无限循环时，我们会及时发现。

**恒定的内存使用**。
与上一条类似，任何地方都没有动态分配的内存，所以也几乎不存内存数据交换到磁盘、或者内存泄漏的可能性。

**它具有高度的可移植性**。
与经典的二进制文件或脚本相比，一个矩阵乘法序列更容易在不同的计算配置上运行。

**它是非常敏捷**。
如果你有一个 C++ 代码，而有人想让你把它的速度提高一倍（如果需要的话，以牺牲性能为代价），那么为新的规范调整系统将是非常巨大的工作量的。
然而，在软件 2.0 中，我们可以把我们的网络，去掉一半的管道，重新训练，然后它的运行速度正好是原来的两倍，当然工作效果要差一点。
这可真神奇。
相反，如果你碰巧得到了更多的数据/计算，你可以立即让你的程序工作得更好，只需增加更多的网络管道并重新训练。

**模块可以融合成一个最佳的整体**。
我们的软件经常被分解成模块，通过公共函数、API 或端点进行通信。
然而，如果两个原本分开训练的软件 2.0 模型进行交互，我们可以很容易地通过反向传播的方法把它们进行融合。
想一想，如果你的 Web 浏览器接收到 10 个操作，然后它会自动重新设计低层的系统指令，以达到更高的网页加载效率，那该有多神奇。
而在 2.0 的环境下，这种行为是默认的。

**它比你更好**。
最后，也是最重要的一点，在很大一部分有价值的垂直领域，神经网络比你我能想到的任何东西都要好，目前至少涉及到任何与图像/视频和声音/语音有关的领域。

## 软件 2.0 的局限性
2.0 也有自己的一些缺点。
在优化好一个模型后，我们留下的大型网络很好用，但理解内部原理却很难。
在很多应用领域，我们会选择使用我们能理解的 90% 准确率的模型，但是对于那些我们不理解的模型，可能它的准确性需要达到 99% ，我们才会用它。

2.0 可能会以不直观和令人尴尬的方式失败，更糟糕的是，它们可能会“[无声地失败](https://motherboard.vice.com/en_us/article/nz7798/weve-already-taught-artificial-intelligence-to-be-racist-sexist)”，例如，通过在训练数据中默默地加入偏差，当它们训练数据的的规模达到数百万时，就很难对模型进行正确分析和检查。

最后，我们还在发现软件 2.0 的一些奇特属性。
例如，一些针对 2.0 安全性的攻击（如，“[对抗性攻击](https://blog.openai.com/adversarial-example-research/)”、[更多](https://github.com/yenchenlin/awesome-adversarial-machine-learning)）显得很奇特，让我们看到这个堆栈的具有很大的不直观性。

## 在2.0栈中编程
软件 1.0 是我们编写的代码。
软件 2.0 是根据评价标准（比如“正确地分类这个训练数据”）编写的优化代码。
该程序的运作原理可能不很明显，但是可以通过反复评估它的性能，来调整模型的任何参数设置以优化模型的表现（例如--你是否正确地分类了一些图像？你是否赢得了围棋比赛?），因为优化过程可以帮助你找到比人类所能写出的更好的代码。

我们看待趋势的视角很重要。
如果你认识到软件 2.0 是一种新兴的编程范式，而不是简单地把神经网络当作机器学习技术中一个好用的分类器，那么向外扩展就会变得更加简单，不过，还有更多的工作要做。

特别是，我们已经建立了大量辅助人类编写 1.0 代码的工具，比如强大的 IDE，具有语法高亮、调试器、剖析器、函数跳转、git 集成等功能。
在 2.0 堆栈中，我们通过收集、调整和清洗数据集来完成编程。
例如，当网络在一些难点或罕见的情况下失败时，我们并不是通过写代码来修复这些预测，而是通过加入更多的这些类似情况下的标签化例子来进一步训练模型。
谁会开发第一个软件 2.0 的 IDE 来帮助收集、可视化、清洗、标注和采购数据集的所有工作流程？
也许 IDE 会根据每个例子的损失，自动凸显出网络怀疑是错误标签的图像，或者通过用预测种子标签来协助过程，或者根据网络预测的不确定性，建议对有用的例子进行标注。

Github 是一个非常成功的软件 1.0 代码托管平台。
在软件 2.0 中类似 Github 的托管平台有发展空间吗？
可能在这种情况下，仓库中存放的是数据集，而用户可以对仓库中的数据进行标注或修改标签。

从短期/中期来看，在某些领域中，重复评估结果的好坏是可能的，也是便宜的，而且算法本身很难明确设计，那么软件 2.0 将会在这些领域中越来越普及。
而从更长远的角度来看，这种范式的未来是光明的，因为很多人越来越清楚，当我们开发 AGI（**Artificial general intelligence**，**通用人工智能**，啥都能干，像人一样）时，肯定会用软件 2.0 来编写。

---
[返回目录](https://github.com/datugou/Article_Translation/tree/master/LEARNING_data_science)
